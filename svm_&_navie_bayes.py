# -*- coding: utf-8 -*-
"""SVM & Navie bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3sMmqIM1aKvtnjj83a_5Ft5HOF70-kD

#SVM and Naive Bayes Assignment

###1. What is a Support Vector Machine (SVM)?

- A Support Vector Machine (SVM) is a powerful, supervised machine learning algorithm used for classification, regression, and outlier detection. Its primary goal is to find the optimal decision boundary, called a hyperplane, that maximally separates different classes of data points in a high-dimensional space.

###
2. What is the difference between Hard Margin and Soft Margin SVM?

- The main difference is that a Hard Margin SVM requires perfect linear separation of the data without any misclassifications, while a Soft Margin SVM allows some misclassifications to achieve a more generalized and robust model.

###3. What is the mathematical intuition behind SVM?

-> Core Idea of SVM

To find a decision boundary (hyperplane) that separates classes with the maximum margin.

A margin is the distance between the hyperplane and the closest data points (called support vectors).
SVM chooses the hyperplane that leaves the largest possible safety gap between classes.

#### Why maximize the margin?
Because a larger margin means:<br>
- Better generalization to new unseen data
- More robust to noise
- Lower risk of overfitting

Mathematically, maximizing margin = minimizing the norm of weight vector ‚Äñw‚Äñ.

#### Mathematical Formulation
Hyperplane equation:
wTx+b=0

#####For a correct classifier:
yi(wTxi+b)‚â•1  ‚àÄi

Margin = 2\‚à•w‚à•

So maximizing margin = minimizing ‚à•w‚à•

####Optimization Problem:
Min 1/2(‚à•w‚à•)^2

Subject to:

yi‚Äã(wTxi‚Äã+b)‚â•1

This is a convex optimization problem with a unique global optimum ‚Äî a huge advantage!

###4. What is the role of Lagrange Multipliers in SVM?

Lagrange multipliers convert the constrained optimization of SVM into a dual form.<br>
This helps:

- Enforce margin constraints using multipliers
ùõº
ùëñ

- Identify support vectors (where ùõºùëñ>0)

- Enable the kernel trick by representing the model using dot products only

Thus, they allow efficient optimization and non-linear SVM.

###5. What are Support Vectors in SVM?

Support vectors are the data points closest to the decision boundary (hyperplane).
They are crucial because they define the position and margin of the classifier.
Removing them can change the model significantly.

###6. What is a Support Vector Classifier (SVC)?
Support Vector Classifier is the SVM model used for classification tasks.
It finds the optimal separating hyperplane that maximizes the margin between classes.

###7. What is the Kernel Trick in SVM?

Kernel Trick allows SVM to transform non-linearly separable data into a higher-dimensional space without explicitly computing transformations.

This helps SVM classify data with curved boundaries.

###8. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?

Comparison: Linear Kernel vs Polynomial Kernel vs RBF Kernel
| Kernel             | When to Use                      | Pros                     | Cons                         |
| ------------------ | -------------------------------- | ------------------------ | ---------------------------- |
| **Linear**         | Data is linearly separable       | Fast, simple             | Poor for complex patterns    |
| **Polynomial**     | Data with interactions           | Can fit curved relations | More computational cost      |
| **RBF (Gaussian)** | Most real-world non-linear cases | Very flexible & powerful | Sensitive to hyperparameters |

###9. What is the effect of the C parameter in SVM?

Effect of the C Parameter in SVM
| C Value    | Impact                                                                        |
| ---------- | ----------------------------------------------------------------------------- |
| **High C** | Less margin, tries to classify all points correctly ‚Üí **risk of overfitting** |
| **Low C**  | Larger margin, allows misclassification ‚Üí **better generalization**           |

###10. Effect of the C Parameter in SVM?

| C Value    | Impact                                                                        |
| ---------- | ----------------------------------------------------------------------------- |
| **High C** | Less margin, tries to classify all points correctly ‚Üí **risk of overfitting** |
| **Low C**  | Larger margin, allows misclassification ‚Üí **better generalization**           |

###11.  What is the role of the Gamma parameter in RBF Kernel SVM?

Gamma controls how far influence of a training point reaches.
| Gamma Value    | Effect                                                          |
| -------------- | --------------------------------------------------------------- |
| **High Gamma** | Very close influence ‚Üí highly curved boundary ‚Üí **overfitting** |
| **Low Gamma**  | Far influence ‚Üí smoother boundary ‚Üí **underfitting possible**   |

###12. What is the Na√Øve Bayes classifier, and why is it called "Na√Øve"?


It is a probabilistic classifier based on Bayes‚Äô Theorem assuming features are independent.

Called na√Øve because in reality, features are rarely fully independent.

###13. What is Bayes‚Äô Theorem?
Bayes‚Äô Theorem
ùëÉ
(
ùê¥
‚à£
ùêµ
)
=
ùëÉ
(
ùêµ
‚à£
ùê¥
)
‚ãÖ
ùëÉ
(
ùê¥
)
ùëÉ
(
ùêµ
)
P(A‚à£B)=
P(B)
P(B‚à£A)‚ãÖP(A)
	‚Äã


Used to calculate the probability of class A given evidence B.

###14.  Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes?

Differences between Gaussian, Multinomial & Bernoulli Na√Øve Bayes
| Variant                     | Type of Data                                | What it Assumes                                                   | Where it is Used                                                              |
| --------------------------- | ------------------------------------------- | ----------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| **Gaussian Na√Øve Bayes**    | Continuous numerical features (real values) | Features are distributed using **Normal (Gaussian) Distribution** | Medical data, sensor data, iris dataset, continuous features classification   |
| **Multinomial Na√Øve Bayes** | Count-based features                        | Features represent frequencies or **word counts**                 | Text classification (document classification, spam detection, topic modeling) |
| **Bernoulli Na√Øve Bayes**   | Binary/Boolean features                     | Feature is **present (1)** or **absent (0)**                      | Email spam (word present or not), sentiment analysis, binary text features    |

###15. When should you use Gaussian Na√Øve Bayes over other variants?

####When to you use Gaussian Na√Øve Bayes:

Use Gaussian Na√Øve Bayes when your dataset contains continuous numerical features that roughly follow a normal (Gaussian) distribution.

####Recommended when:

- Data includes measurements like:

    Height, weight, temperature, blood pressure

    Sensor readings

    Age, time, financial values

- You are working with scientific or medical datasets

- Features are not binary or count-based

###16. What are the key assumptions made by Na√Øve Bayes?

Na√Øve Bayes makes two main assumptions:

1 Feature Independence Assumption:

All features are independent of each other given the class label

Example: In email spam detection, the presence of the word ‚Äúfree‚Äù does not affect ‚Äúwin‚Äù (according to NB)

2 Feature Contribution Assumption:

Each feature contributes independently and equally to the probability of the outcome.

###17. What are the advantages and disadvantages of Na√Øve Bayes?

####Advantages

- Fast and efficient ‚Äî works well with large datasets

- Good performance even with small training data

- Works well for high-dimensional data like text

- Simple and easy to implement

* Handles both continuous and categorical data

- Not sensitive to irrelevant features

#### Disadvantages

- Strong independence assumption rarely true in real world

- May perform poorly if features are highly correlated

- Zero probability problem if a word/feature doesn‚Äôt appear in training
‚Üí Fixed using Laplace Smoothing

- Decision boundaries are linear (less flexible than SVM)

### 18. Why is Na√Øve Bayes a good choice for text classification?

Na√Øve Bayes is ideal for text because:

- Text data is high dimensional ‚Üí NB handles thousands of features easily

- Word occurrences are treated as independent features ‚Üí aligns well with NB assumption

- Very fast training & prediction ‚Üí great for large corpora

- Performs well even with sparse data (many zeros)

- Commonly used in:

   Spam detection

    Sentiment analysis

    Document/topic classification

In most NLP tasks, Multinomial Na√Øve Bayes works best.

###19. Compare SVM and Na√Øve Bayes for classification tasks.

| Feature                        | SVM                            | Na√Øve Bayes                       |
| ------------------------------ | ------------------------------ | --------------------------------- |
| Type                           | Discriminative                 | Generative                        |
| Data type                      | Continuous, complex boundaries | Text & categorical data           |
| Training time                  | Slower, computationally heavy  | Very fast                         |
| Performance                    | High accuracy in many tasks    | Good baseline, fast               |
| Handling high-dimensional data | Good with kernel trick         | Excellent for text (bag-of-words) |
| Probabilistic output           | No (needs calibration)         | Yes                               |
| Noise handling                 | Sensitive to noise             | More robust to limited noise      |

For text classification : Na√Øve Bayes

For complex decision boundaries : SVM

###20 How does Laplace Smoothing help in Na√Øve Bayes?

Laplace Smoothing prevents zero probability issues in Na√Øve Bayes.

If a word never appears in training data for a class ‚Üí its probability becomes 0
‚Üí entire product becomes 0 ‚Üí wrong classification

Formula:

ùëÉ
(
ùë•
ùëñ
‚à£
ùëå
)
= count(
ùë•
ùëñ
,
ùëå) +
1/
Total words in class
+
V

Where v is vocabulary size.

####Effect:

- Ensures no probability becomes zero

- Helps the model generalize better for unseen words/features

#Practical questions.

21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVM model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

"""22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then
compare their accuracies
"""

from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
data = load_wine()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Linear SVM
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))

# RBF SVM
svm_rbf = SVC(kernel='rbf')
svm_rbf.fit(X_train, y_train)
rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))

print(f"Linear Kernel Accuracy: {linear_acc:.4f}")
print(f"RBF Kernel Accuracy: {rbf_acc:.4f}")

"""23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean
Squared Error (MSE).
"""

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
import numpy as np

# Load dataset (California housing ‚Äî replacement of Boston dataset)
data = fetch_california_housing()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVR model
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)

# Predictions & MSE
y_pred = svr_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")

"""24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision
boundary.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Load Iris dataset (only first 2 features)
iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

# Train Polynomial SVM
model = SVC(kernel='poly', degree=3, C=1.0)
model.fit(X, y)

# Plot decision boundary
x_min, x_max = X[:,0].min()-1, X[:,0].max()+1
y_min, y_max = X[:,1].min()-1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X[:,0], X[:,1], c=y, s=50, edgecolors='k')
plt.title("SVM with Polynomial Kernel (Degree 3)")
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.show()

"""25. Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and
evaluate accuracy.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc:.4f}")

"""26. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20
Newsgroups dataset.
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Load dataset
train_data = fetch_20newsgroups(subset='train')
test_data = fetch_20newsgroups(subset='test')

# Feature extraction
vectorizer = CountVectorizer(stop_words='english')
X_train = vectorizer.fit_transform(train_data.data)
X_test = vectorizer.transform(test_data.data)
y_train, y_test = train_data.target, test_data.target

# Multinomial NB
model = MultinomialNB()
model.fit(X_train, y_train)

# Predictions & accuracy
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc:.4f}")

"""27. Write a Python program to train an SVM Classifier with different C values and compare the decision
boundaries visually.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Load Iris dataset (only first 2 features for easy visualization)
iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

# Different C values to compare
C_values = [0.01, 0.1, 1, 10]

plt.figure(figsize=(15, 10))

for index, C in enumerate(C_values, 1):
    # Train SVM model with given C
    model = SVC(kernel="linear", C=C)

"""28.  Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with
binary features.
"""

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Convert continuous features to binary (threshold-based)
X_binary = np.where(X > np.mean(X, axis=0), 1, 0)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2, random_state=42)

# Bernoulli Naive Bayes model
model = BernoulliNB()
model.fit(X_train, y_train)

# Predictions & accuracy
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc:.4f}")

"""29. Write a Python program to apply feature scaling before training an SVM model and compare results with
unscaled data.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap

# Load dataset (binary classification)
X, y = datasets.make_blobs(n_samples=300, centers=2, random_state=42, cluster_std=2.0)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Helper function for plotting decision boundary
def plot_decision_boundary(model, X, y, title):
    plt.figure(figsize=(6, 4))
    ax = plt.gca()

    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1
    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1

    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),
                         np.arange(y_min, y_max, 0.05))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['pink', 'lightblue']))
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['red', 'blue']))
    ax.set_title(title)
    plt.show()

# ---- Train WITHOUT Scaling ----
svm_unscaled = SVC(kernel='linear')
svm_unscaled.fit(X_train, y_train)

y_pred_unscaled = svm_unscaled.predict(X_test)
acc_unscaled = accuracy_score(y_test, y_pred_unscaled)
print("Accuracy without scaling:", acc_unscaled)

plot_decision_boundary(svm_unscaled, X_test, y_test, "Decision Boundary - Without Scaling")

# ----  Train WITH Scaling ----
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_scaled = SVC(kernel='linear')
svm_scaled.fit(X_train_scaled, y_train)

y_pred_scaled = svm_scaled.predict(X_test_scaled)
acc_scaled = accuracy_score(y_test, y_pred_scaled)
print("Accuracy with scaling:", acc_scaled)

# Plot on scaled feature space
plot_decision_boundary(svm_scaled, X_test_scaled, y_test, "Decision Boundary - With Scaling")

"""30. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and
after Laplace Smoothing.

Gaussian Na√Øve Bayes does not use Laplace smoothing:


    Laplace smoothing (Œ± / additive smoothing) is only meaningful for probability counts (categorical / multinomial data).

Gaussian NB estimates mean and variance of continuous features, not discrete counts ‚Äî so Laplace doesn‚Äôt apply.


However‚Ä¶
Scikit-learn‚Äôs GaussianNB does have a parameter:
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ---- Model 1: Without additional smoothing ----
gnb_default = GaussianNB()
gnb_default.fit(X_train, y_train)

pred_default = gnb_default.predict(X_test)
acc_default = accuracy_score(y_test, pred_default)
print("Accuracy (Default GaussianNB):", acc_default)
print("Predictions (Default):", pred_default)

# ---- Model 2: With Increased Smoothing ----
gnb_smoothed = GaussianNB(var_smoothing=1e-2)  # larger smoothing
gnb_smoothed.fit(X_train, y_train)

pred_smoothed = gnb_smoothed.predict(X_test)
acc_smoothed = accuracy_score(y_test, pred_smoothed)
print("\nAccuracy (With Smoothing):", acc_smoothed)
print("Predictions (Smoothed):", pred_smoothed)

# Compare differences
print("\nPrediction changes due to smoothing:")
print(pred_default - pred_smoothed)

"""31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,
gamma, kernel).
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
iris = datasets.load_iris()
X, y = iris.data, iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Parameter grid for tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly']
}

# Create SVM model and GridSearchCV
svm_model = SVC()
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the model
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

# Evaluate the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and
check it improve accuracy.
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter

# Create an imbalanced dataset (90% of class 0, 10% of class 1)
X, y = make_classification(n_samples=2000, n_features=10, n_informative=5,
                           n_redundant=2, weights=[0.9, 0.1], random_state=42)

print("Class Distribution:", Counter(y))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 1 SVM WITHOUT Class Weighting

svm_default = SVC(kernel='rbf', random_state=42)
svm_default.fit(X_train, y_train)
y_pred_default = svm_default.predict(X_test)

print("\n Results WITHOUT Class Weighting ")
print("Accuracy:", accuracy_score(y_test, y_pred_default))
print(classification_report(y_test, y_pred_default))


# 2 SVM WITH Class Weighting

svm_weighted = SVC(kernel='rbf', class_weight='balanced', random_state=42)
svm_weighted.fit(X_train, y_train)
y_pred_weighted = svm_weighted.predict(X_test)

print("\n Results WITH Class Weighting ")
print("Accuracy:", accuracy_score(y_test, y_pred_weighted))
print(classification_report(y_test, y_pred_weighted))

"""33. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data.

"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Load email-like dataset (using 2 spam-related categories for simplicity)
categories = ['rec.sport.hockey', 'talk.politics.misc']  # Consider 1 as 'spam'
data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)

# Features (text) and labels (spam or ham)
X = data.data
y = data.target

# Convert text ‚Üí Bag of Words
vectorizer = CountVectorizer(stop_words='english')
X_vec = vectorizer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.25, random_state=42)

# Na√Øve Bay

"""34. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and
compare their accuraccy.
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load dataset
data = datasets.load_breast_cancer()
X, y = data.data, data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# SVM Classifier

svm_model = SVC(kernel='rbf', gamma='scale')
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)
svm_acc = accuracy_score(y_test, svm_pred)

# Na√Øve Bayes Classifier

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)
nb_acc = accuracy_score(y_test, nb_pred)

# Print comparison results
print(" Accuracy Comparison ")
print(f"SVM Accuracy: {svm_acc:.4f}")
print(f"Na√Øve Bayes Accuracy: {nb_acc:.4f}")

"""35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare
results.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import accuracy_score

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 1 Na√Øve Bayes WITHOUT Feature Selection

nb_full = GaussianNB()
nb_full.fit(X_train, y_train)
y_pred_full = nb_full.predict(X_test)
acc_full = accuracy_score(y_test, y_pred_full)


# 2 Feature Selection (Top K features)

k = 10  # number of best features to keep

selector = SelectKBest(score_func=chi2, k=k)
X_new = selector.fit_transform(X, y)

X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(
    X_new, y, test_size=0.3, random_state=42
)

nb_fs = GaussianNB()
nb_fs.fit(X_train_fs, y_train_fs)
y_pred_fs = nb_fs.predict(X_test_fs)
acc_fs = accuracy_score(y_test_fs, y_pred_fs)

# Print results
print(" Na√Øve Bayes Accuracy WITHOUT Feature Selection :", round(acc_full, 4))
print(" Na√Øve Bayes Accuracy WITH Feature Selection    :", round(acc_fs, 4))
print(f"\nSelected Top {k} Feature Indices:", selector.get_support(indices=True))

"""36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)
strategies on the Wine dataset and compare their accuracy.
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier
from sklearn.metrics import accuracy_score

# Load Wine dataset
wine = datasets.load_wine()
X, y = wine.data, wine.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 1 One-vs-Rest (OvR)

ovr_model = OneVsRestClassifier(SVC(kernel='rbf', gamma='scale'))
ovr_model.fit(X_train, y_train)
y_pred_ovr = ovr_model.predict(X_test)
acc_ovr = accuracy_score(y_test, y_pred_ovr)


# 2 One-vs-One (OvO)

ovo_model = OneVsOneClassifier(SVC(kernel='rbf', gamma='scale'))
ovo_model.fit(X_train, y_train)
y_pred_ovo = ovo_model.predict(X_test)
acc_ovo = accuracy_score(y_test, y_pred_ovo)

# Print comparison results
print(" Accuracy Comparison on Wine Dataset ")
print(f"One-vs-Rest (OvR) Accuracy : {acc_ovr:.4f}")
print(f"One-vs-One  (OvO) Accuracy : {acc_ovo:.4f}")

"""37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast
Cancer dataset and compare their accuracy.
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load dataset
data = datasets.load_breast_cancer()
X, y = data.data, data.target

# Feature Scaling (Important for SVM!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)


# SVM with different kernels


# Linear Kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_test)
acc_linear = accuracy_score(y_test, y_pred_linear)

# Polynomial Kernel
svm_poly = SVC(kernel='poly', degree=3)
svm_poly.fit(X_train, y_train)
y_pred_poly = svm_poly.predict(X_test)
acc_poly = accuracy_score(y_test, y_pred_poly)

# RBF Kernel
svm_rbf = SVC(kernel='rbf')
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_test)
acc_rbf = accuracy_score(y_test, y_pred_rbf)

# Print comparison
print(" Accuracy Comparison (Breast Cancer Dataset) ")
print(f"Linear Kernel Accuracy  : {acc_linear:.4f}")
print(f"Polynomial Kernel Acc. : {acc_poly:.4f}")
print(f"RBF Kernel Accuracy    : {acc_rbf:.4f}")

"""38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the
average accuracy.
"""

from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import numpy as np

# Load dataset (Breast Cancer)
data = datasets.load_breast_cancer()
X, y = data.data, data.target

# Create SVM model pipeline (includes scaling)
model = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='scale'))

# Stratified K-Fold Cross Validation (ensures balanced class splits)
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Perform cross validation
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')

print("Cross-Validation Accuracies:", np.round(scores, 4))
print("\nAverage Accuracy:", round(scores.mean(), 4))
print("Standard Deviation:", round(scores.std(), 4))

"""39. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare
performance.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# -------------------------------------
# Model 1: Default prior probabilities
# -------------------------------------
nb_default = GaussianNB()
nb_default.fit(X_train, y_train)
pred_default = nb_default.predict(X_test)
acc_default = accuracy_score(y_test, pred_default)

# -------------------------------------
# Model 2: Custom prior probabilities
# Example: Give more weight to class 1
# -------------------------------------
custom_priors = [0.3, 0.7]  # Adjust as desired

nb_custom = GaussianNB(priors=custom_priors)
nb_custom.fit(X_train, y_train)
pred_custom = nb_custom.predict(X_test)
acc_custom = accuracy_score(y_test, pred_custom)

# -------------------------------------
# Results
# -------------------------------------
print(" Comparison of Na√Øve Bayes Performance \n")
print("Default Priors:")
print(f"Accuracy: {acc_default:.4f}")
print(classification_report(y_test, pred_default))

print("\nCustom Priors:")
print(f"Accuracy: {acc_custom:.4f}")
print(classification_report(y_test, pred_custom))

"""40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and
compare accuracy.
"""

# Recursive Feature Elimination (RFE) with SVM Classifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split into train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model on all features
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
y_pred_all = svm.predict(X_test)
accuracy_all = accuracy_score(y_test, y_pred_all)

# Feature Selection using RFE
num_features = 10  # Choose number of features to select
rfe = RFE(estimator=svm, n_features_to_select=num_features)
rfe.fit(X_train, y_train)

# Transform train and test sets to selected features only
X_train_rfe = rfe.transform(X_train)
X_test_rfe = rfe.transform(X_test)

# Train model with selected features
svm_rfe = SVC(kernel='linear')
svm_rfe.fit(X_train_rfe, y_train)
y_pred_rfe = svm_rfe.predict(X_test_rfe)
accuracy_rfe = accuracy_score(y_test, y_pred_rfe)

# Print Results
print("Total Features:", X.shape[1])
print("Selected Features:", num_features)
print("\nAccuracy without Feature Selection: {:.4f}".format(accuracy_all))
print("Accuracy with RFE Feature Selection: {:.4f}".format(accuracy_rfe))

# Display the selected features
print("\nSelected Feature Indices:", rfe.get_support(indices=True))

"""41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and
F1-Score instead of accuracy.
"""

# SVM Classification with Precision, Recall, and F1-Score

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train SVM model
svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Evaluation Metrics
precision = precision_score(y_test, y_pred, average='binary')
recall = recall_score(y_test, y_pred, average='binary')
f1 = f1_score(y_test, y_pred, average='binary')

print("Precision Score:", round(precision, 4))
print("Recall Score:", round(recall, 4))
print("F1 Score:", round(f1, 4))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""42. Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss
(Cross-Entropy Loss).
"""

# Na√Øve Bayes Classification - Log Loss (Cross-Entropy Loss)

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import log_loss

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train Gaussian Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict probabilistic outputs (required for Log Loss)
y_prob = model.predict_proba(X_test)

# Calculate Log Loss
loss = log_loss(y_test, y_prob)

print("Log Loss (Cross-Entropy Loss):", loss)

"""43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn."""

# SVM Classifier with Confusion Matrix Visualization (Seaborn)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report

# Load Dataset (Iris)
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Train-test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train SVM Classifier
model = SVC(kernel='rbf')
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Visualization
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title("Confusion Matrix - SVM Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Display Classification Report (Optional)
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

"""44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute
Error (MAE) instead of MSE.
"""

# Importing required libraries
from sklearn.datasets import fetch_california_housing
import numpy
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

# Loading datasets
data = fetch_california_housing()
x = data.data
y = data.target

# Split data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

# Feature Scaling
scaler_x = StandardScaler()
scaler_y = StandardScaler()

x_train = scaler_x.fit_transform(x_train)
x_test = scaler_x.transform(x_test)

y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()

# Train SVR model
svr = SVR(kernel='rbf')
svr.fit(x_train, y_train)

# Predictions
y_pred = svr.predict(x_test)
y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).ravel()

# Evaluating using MAE
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE):", mae)

"""45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC
score.
"""

# Importing required libraries
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (Naive Bayes works better with normalized data)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict probabilities for the positive class
y_prob = model.predict_proba(X_test)[:, 1]

# Evaluate using ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_prob)
print("ROC-AUC Score:", roc_auc)

"""46 Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."""

# Importing required libraries
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train SVM Classifier (enable probability estimates for PR curve)
model = SVC(kernel='rbf', probability=True)
model.fit(X_train, y_train)

# Predicted probabilities for positive class
y_prob = model.predict_proba(X_test)[:, 1]

# Compute Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall, precision)

# Print AUC Score
print("Precision-Recall AUC Score:", pr_auc)

# Plot Precision-Recall curve
plt.figure()
plt.plot(recall, precision, linewidth=2)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve (AUC = {:.3f})".format(pr_auc))
plt.grid(True)
plt.show()